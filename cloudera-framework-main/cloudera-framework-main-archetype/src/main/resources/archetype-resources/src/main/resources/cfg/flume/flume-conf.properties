###############################################################################
#
# Flume / Kafka ingest pipeline, offering 3 broadcast channels that can be 
# tapped with differing qualities of service:
#
#   1. single/non-durable, optimised for lowest latency delivery of single events,
#      add memory channel to mydataset.sources.source_single.channels
#   2. single/durable, optimised for low latency delivery of single events with
#      durable operation, add kafka source to topic mydataset_single
#   3. batch/durable, optimised for greatest throughput of batch events with
#      durable operation, add kafka source to topic mydataset_batch
#
###############################################################################

mydataset.sources=source_single source_batch
mydataset.channels=channel_single_nondurable_hdfs channel_single_durable channel_batch_durable
mydataset.sinks=sink_single_hdfs sink_batch_hdfs

mydataset.sources.source_single.type=com.cloudera.example.stream.Stream
mydataset.sources.source_single.recordType=TEXT_CSV
mydataset.sources.source_single.pollMs=1000
mydataset.sources.source_single.pollTicks=1
mydataset.sources.source_single.batchSize=1
mydataset.sources.source_single.recordNumber=50
mydataset.sources.source_single.interceptors=interceptor_stream
mydataset.sources.source_single.interceptors.interceptor_stream.type=com.cloudera.example.stream.Stream$Interceptor$Builder
mydataset.sources.source_single.selector.type=replicating
mydataset.sources.source_single.channels=channel_single_nondurable_hdfs channel_single_durable

mydataset.channels.channel_single_nondurable_hdfs.type=memory
mydataset.channels.channel_single_nondurable_hdfs.capacity=10000
mydataset.channels.channel_single_nondurable_hdfs.transactionCapacity=1000
mydataset.channels.channel_single_nondurable_hdfs.keep-alive=1
mydataset.channels.channel_single_nondurable_hdfs.byteCapacityBufferPercentage=1

mydataset.sinks.sink_single_hdfs.type=hdfs
mydataset.sinks.sink_single_hdfs.hdfs.path=$HDFS_ROOT/$ROOT_DIR_HDFS_RAW/source/text/%{type}/none/%{batch}_mydataset-%{agent}.%{type}
mydataset.sinks.sink_single_hdfs.hdfs.filePrefix=%{timestamp}_mydataset-%{index}-of-%{total}
mydataset.sinks.sink_single_hdfs.hdfs.fileSuffix=
mydataset.sinks.sink_single_hdfs.hdfs.inUsePrefix=_
mydataset.sinks.sink_single_hdfs.hdfs.rollCount=0
mydataset.sinks.sink_single_hdfs.hdfs.rollInterval=0
mydataset.sinks.sink_single_hdfs.hdfs.rollSize=0
mydataset.sinks.sink_single_hdfs.hdfs.idleTimeout=1
mydataset.sinks.sink_single_hdfs.hdfs.batchSize=1
mydataset.sinks.sink_single_hdfs.hdfs.writeFormat=Text
mydataset.sinks.sink_single_hdfs.hdfs.fileType=DataStream
mydataset.sinks.sink_single_hdfs.channel=channel_single_nondurable_hdfs

mydataset.channels.channel_single_durable.type=org.apache.flume.channel.kafka.KafkaChannel
mydataset.channels.channel_single_durable.brokerList=$KAFKA_KAFKA_BROKER_HOSTS_AND_PORTS
mydataset.channels.channel_single_durable.zookeeperConnect=$ZOOKEEPER_SERVER_HOSTS_AND_PORTS
mydataset.channels.channel_single_durable.capacity=10000
mydataset.channels.channel_single_durable.transactionCapacity=1000
mydataset.channels.channel_single_durable.topic=mydataset_single

mydataset.sources.source_batch.type=org.apache.flume.source.kafka.KafkaSource
mydataset.sources.source_batch.zookeeperConnect=$ZOOKEEPER_SERVER_HOSTS_AND_PORTS
mydataset.sources.source_batch.groupId=source_batch
mydataset.sources.source_batch.topic=mydataset_single
mydataset.sources.source_batch.batchSize=10
mydataset.sources.source_batch.batchDurationMillis=700000
mydataset.sources.source_batch.interceptors=interceptor_unwrap interceptor_stream
mydataset.sources.source_batch.interceptors.interceptor_unwrap.type=com.cloudera.framework.main.common.flume.FlumeEventUnwrapInterceptor$Builder
mydataset.sources.source_batch.interceptors.interceptor_stream.type=com.cloudera.example.stream.Stream$Interceptor$Builder
mydataset.sources.source_batch.channels=channel_batch_durable

mydataset.channels.channel_batch_durable.type=org.apache.flume.channel.kafka.KafkaChannel
mydataset.channels.channel_batch_durable.brokerList=$KAFKA_KAFKA_BROKER_HOSTS_AND_PORTS
mydataset.channels.channel_batch_durable.zookeeperConnect=$ZOOKEEPER_SERVER_HOSTS_AND_PORTS
mydataset.channels.channel_batch_durable.capacity=10000
mydataset.channels.channel_batch_durable.transactionCapacity=1000
mydataset.channels.channel_batch_durable.topic=mydataset_batch

mydataset.sinks.sink_batch_hdfs.type=hdfs
mydataset.sinks.sink_batch_hdfs.hdfs.path=$HDFS_ROOT/$ROOT_DIR_HDFS_RAW/source/sequence/%{type}/none/%{batch}_mydataset-%{agent}.seq
mydataset.sinks.sink_batch_hdfs.hdfs.filePrefix=%{batch}_mydataset
mydataset.sinks.sink_batch_hdfs.hdfs.fileSuffix=
mydataset.sinks.sink_batch_hdfs.hdfs.inUsePrefix=_
mydataset.sinks.sink_batch_hdfs.hdfs.rollCount=0
mydataset.sinks.sink_batch_hdfs.hdfs.rollInterval=0
mydataset.sinks.sink_batch_hdfs.hdfs.rollSize=0
mydataset.sinks.sink_batch_hdfs.hdfs.idleTimeout=10
mydataset.sinks.sink_batch_hdfs.hdfs.batchSize=10
mydataset.sinks.sink_batch_hdfs.hdfs.writeFormat=Writable
mydataset.sinks.sink_batch_hdfs.hdfs.fileType=SequenceFile
mydataset.sinks.sink_batch_hdfs.channel=channel_batch_durable
