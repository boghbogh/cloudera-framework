###############################################################################
#
# Flume / Kafka ingest pipeline, offering 3 broadcast channels that can be 
# tapped with differing qualities of service:
#
#   1. single/non-durable, optimised for lowest latency delivery of single events,
#      add memory channel to mydataset.sources.source_single.channels
#   2. single/durable, optimised for low latency delivery of single events with
#      durable operation, add kafka channel with topic mydataset_single to
#      mydataset.sources.source_single.channels
#   3. batch/durable, optimised for greatest throughput of batch events with
#      durable operation, add kafka channel with topic mydataset_batch to 
#      mydataset.sources.source_batch.channels
#
###############################################################################

mydataset.sources=source_single source_batch
mydataset.channels=channel_single_durable channel_batch_durable
mydataset.sinks=sink_batch_hdfs

mydataset.sources.source_single.type=com.cloudera.example.stream.Stream
mydataset.sources.source_single.recordType=$RECORD_FORMAT
mydataset.sources.source_single.pollMs=1000
mydataset.sources.source_single.pollTicks=1
mydataset.sources.source_single.batchSize=1
mydataset.sources.source_single.recordNumber=1000
mydataset.sources.source_single.interceptors=interceptor_stream
mydataset.sources.source_single.interceptors.interceptor_stream.type=com.cloudera.example.stream.Stream$Interceptor$Builder
mydataset.sources.source_single.selector.type=replicating
mydataset.sources.source_single.channels=channel_single_durable

mydataset.channels.channel_single_durable.type=org.apache.flume.channel.kafka.KafkaChannel
mydataset.channels.channel_single_durable.brokerList=$KAFKA_KAFKA_BROKER_HOSTS_AND_PORTS
mydataset.channels.channel_single_durable.zookeeperConnect=$ZOOKEEPER_SERVER_HOSTS_AND_PORTS
mydataset.channels.channel_single_durable.capacity=10000
mydataset.channels.channel_single_durable.transactionCapacity=1000
mydataset.channels.channel_single_durable.groupId=mydataset_single
mydataset.channels.channel_single_durable.topic=mydataset_single

mydataset.sources.source_batch.type=org.apache.flume.source.kafka.KafkaSource
mydataset.sources.source_batch.zookeeperConnect=$ZOOKEEPER_SERVER_HOSTS_AND_PORTS
mydataset.sources.source_batch.groupId=mydataset_single
mydataset.sources.source_batch.topic=mydataset_single
mydataset.sources.source_batch.batchSize=120
mydataset.sources.source_batch.batchDurationMillis=700000
mydataset.sources.source_batch.interceptors=interceptor_unwrap interceptor_stream
mydataset.sources.source_batch.interceptors.interceptor_unwrap.type=com.cloudera.framework.common.flume.FlumeEventUnwrapInterceptor$Builder
mydataset.sources.source_batch.interceptors.interceptor_stream.type=com.cloudera.example.stream.Stream$Interceptor$Builder
mydataset.sources.source_batch.selector.type=replicating
mydataset.sources.source_batch.channels=channel_batch_durable

mydataset.channels.channel_batch_durable.type=org.apache.flume.channel.kafka.KafkaChannel
mydataset.channels.channel_batch_durable.brokerList=$KAFKA_KAFKA_BROKER_HOSTS_AND_PORTS
mydataset.channels.channel_batch_durable.zookeeperConnect=$ZOOKEEPER_SERVER_HOSTS_AND_PORTS
mydataset.channels.channel_batch_durable.capacity=10000
mydataset.channels.channel_batch_durable.transactionCapacity=1000
mydataset.channels.channel_batch_durable.groupId=mydataset_batch
mydataset.channels.channel_batch_durable.topic=mydataset_batch

mydataset.sinks.sink_batch_hdfs.type=hdfs
mydataset.sinks.sink_batch_hdfs.hdfs.path=$ROOT_HDFS/$ROOT_DIR_HDFS_STAGED_CANONICAL/sequence/%{bt}/none/ingest_batch_id=%{bid}/ingest_batch_start=%{btss}/ingest_batch_finish=%{btsf}
mydataset.sinks.sink_batch_hdfs.hdfs.filePrefix=mydataset
mydataset.sinks.sink_batch_hdfs.hdfs.fileSuffix=.$RECORD_FORMAT.seq
mydataset.sinks.sink_batch_hdfs.hdfs.inUsePrefix=_
mydataset.sinks.sink_batch_hdfs.hdfs.rollCount=0
mydataset.sinks.sink_batch_hdfs.hdfs.rollInterval=0
mydataset.sinks.sink_batch_hdfs.hdfs.rollSize=0
mydataset.sinks.sink_batch_hdfs.hdfs.idleTimeout=1
mydataset.sinks.sink_batch_hdfs.hdfs.batchSize=120
mydataset.sinks.sink_batch_hdfs.hdfs.writeFormat=com.cloudera.example.stream.Stream$Serializer$Builder
mydataset.sinks.sink_batch_hdfs.hdfs.fileType=SequenceFile
mydataset.sinks.sink_batch_hdfs.channel=channel_batch_durable
